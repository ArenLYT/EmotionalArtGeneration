{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import diffusers\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import accelerate\n",
    "import pytorch_msssim\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DDPMScheduler, UNet2DModel, DDPMPipeline, DDIMScheduler,StableDiffusionPipeline,DPMSolverMultistepScheduler\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8874d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ArtEmisv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modo(x):\n",
    "    if  x.value_counts()[0]>=sum(x.value_counts())*1.0 :\n",
    "        return pd.Series.mode(x)\n",
    "    else:\n",
    "        return x.value_counts()[:3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emtype(x):\n",
    "    if x.emotion=='sadness':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='fear':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='disgust':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='anger':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='contentment':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='awe':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='amusement':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='excitement':\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"something else\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=df\n",
    "dfemo['emotype']= dfemo.apply(emtype,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d51ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo.groupby([\"art_style\",\"painting\"])[\"emotype\"].agg(modo).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aca6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=type(dfemo.emotype[0])\n",
    "dfemo=dfemo[dfemo[\"emotype\"].apply(lambda x: type(x) !=t )].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo[dfemo.emotype!=\"something else\"].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo['path']= dfemo.apply(lambda x: 'dataset\\\\wikiart\\\\'+x['art_style']+\"\\\\\"+ x['painting']+\".jpg\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frac=1-(dfemo.emotype.value_counts()[1]/dfemo.emotype.value_counts()[0])\n",
    "\n",
    "#dfemo = dfemo.drop(dfemo[dfemo['emotype'] == \"positive\"].sample(frac=frac).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(dfemo.emotype,bins=range(0,3), rwidth=0.8,align=\"left\")\n",
    "plt.title('Histogram of Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c506bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return torchvision.transforms.functional.pad(image, padding, 255, 'constant')\n",
    "\n",
    "# now use it as the replacement of transforms.Pad class\n",
    "transform=torchvision.transforms.Compose([\n",
    "    SquarePad(),\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(256),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=torchvision.datasets.ImageFolder(root=\"diffset\",transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed it into a dataloader (batch size 8 here just for demo)\n",
    "train_dataloader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "# View some examples\n",
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x).permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7630071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=2, class_emb_size=2):\n",
    "    super().__init__()\n",
    "    \n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=256,           # the target image resolution\n",
    "        in_channels=3 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=3,           # the number of output channels\n",
    "        layers_per_block=4,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256,256,512,512), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\", \n",
    "            \"AttnUpBlock2D\", \n",
    "            \"UpBlock2D\",      \n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\", \n",
    "            \"UpBlock2D\",\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dinemsion\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the unet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c31edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=500, beta_schedule='squaredcos_cap_v2')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c635eb-e9b0-4469-a8d4-0cba2150e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=DDPMPipeline.from_pretrained(\"google/ddpm-bedroom-256\").to(\"cuda\")\n",
    "noise_scheduler= DDIMScheduler.from_pretrained(\"google/ddpm-bedroom-256\")\n",
    "noise_scheduler.set_timesteps(num_inference_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
    "train_dataloader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Our network \n",
    "net = ClassConditionedUnet().to(\"cuda\")\n",
    "\n",
    "# Our loss finction\n",
    "loss_fn = pytorch_msssim.SSIM()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-5) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(\"cuda\")  # Data on the GPU (mapped to (-1, 1))\n",
    "        y = y.to(\"cuda\")\n",
    "        noise = torch.randn_like(x)\n",
    "        noise =(noise/10)+0.5\n",
    "        #timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(\"cuda\")\n",
    "        timesteps=torch.randint(0, noise_scheduler.num_train_timesteps, (x.shape[0],)).long().to(\"cuda\")\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "        \n",
    "        # Get the model prediction\n",
    "        \n",
    "        pred = net(noisy_x, timesteps,y) # Note that we pass in the labels y\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(pred,noise)+(1-loss_fn(pred,noise)) # How close is the output to the noise\n",
    "        #loss=F.mse_loss(pred,noise)\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print our the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b90dd-7fe7-4fbf-8cb2-f78a68fd196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net,r\"D://EmotionalArtGeneration//epoch10ssim.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36293a15-a315-460f-a9c0-62c9bf2b09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "netload=torch.load(\"D://EmotionalArtGeneration//epoch10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc1ddb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065916618861410fac9a6ec04a35d8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0dfc41176f4689bf17eabc7332d8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dce97cced446e9a6506e4d77e084d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014906dbe4504ce0ac44b263e453d977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835eea5c83de4697a6c44f45b816b2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052c72bc2db648b0b7a004e21fa7171b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c3e8e86d094a63ab0a0108eceb5020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df8d690a7ac43c18245285a293e2c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d208336de04d5a986dc81e989388bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9faddaf42b194a04a0ba19b8b29bc318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74decc078418419bbe5ba395c1a0c70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29afcab88d444ef581f1f8cee19c6d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ce0e03715c48968726b13145ec0ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4630816255946318d63256498982836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0965c1a023ed47778e03cc4e94fe3273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5061b8cacce6408fb689f6b53f411a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656f8a75e301483ea77782e0359f23f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf31a75665e479b957bbf205db8ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468ba0cb5d5c46dd8c57ffee7412a615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11d97f64b204e0b9468c41948b5dc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(20):\n",
    "    # Prepare random x to start from, plus some desired labels y\n",
    "    x = torch.randn(10, 3, 256, 256).to('cuda')\n",
    "    y = torch.tensor([[i]*5 for i in range(2)]).flatten().to('cuda')\n",
    "    # Sampling loop\n",
    "    for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "    \n",
    "        # Get model pred\n",
    "        with torch.no_grad():\n",
    "            residual = netload(x, t,y)  # Again, note that we pass in our labels y\n",
    "    \n",
    "        # Update sample with step\n",
    "        x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "    z=(x+1)/2\n",
    "    p=z.detach().cpu()\n",
    "    for k in range(p.shape[0]):\n",
    "        #result=Image.fromarray(p[i].permute(1,2,0))\n",
    "        if k<5:\n",
    "            torchvision.utils.save_image(p[k],\"results//negative//\"+str(k+(j*10))+\".png\")\n",
    "            #result.save(\"results//negative//\"+str(i)+\".png\")\n",
    "        else:\n",
    "            torchvision.utils.save_image(p[k],\"results//positive//\"+str(k+j*10)+\".png\")\n",
    "            #result.save(\"results//positive//\"+str(i)+\".png\")\n",
    "        \n",
    "# Show the results\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(48, 48))\n",
    "#ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(0, 255), nrow=8).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb796033-13f1-4597-beef-c9e5a4ee9895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
