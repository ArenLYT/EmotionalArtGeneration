{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import diffusers\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71fa890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= nn.Embedding(10,4)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b8adb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=test(torch.tensor([0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d287424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8315,  0.8875,  0.6745, -3.5290],\n",
       "        [-0.9184,  0.0409,  0.6124,  0.1508],\n",
       "        [ 1.1642,  1.1355, -0.1836,  2.0500],\n",
       "        [ 1.9729, -1.4642, -0.1668, -1.5161],\n",
       "        [ 1.1106,  1.2622,  0.4863,  1.5984],\n",
       "        [-0.5009,  0.9400,  0.3000, -0.8358],\n",
       "        [ 0.1680, -0.0150, -1.9553, -2.6152],\n",
       "        [ 0.0828,  1.3484,  0.5490, -0.6323],\n",
       "        [ 1.3828,  0.3842, -0.3215, -0.4033],\n",
       "        [ 0.7471,  0.3240,  1.8070, -1.6722]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdb22a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[10,4]->[4,2,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3fb7ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a5ce700",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 4, 1, 1]' is invalid for input of size 40",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15036\\3333052189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[8, 4, 1, 1]' is invalid for input of size 40"
     ]
    }
   ],
   "source": [
    "tv=test2.view(8,4,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8874d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ArtEmisv1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modo(x):\n",
    "    if  x.value_counts()[0]>=sum(x.value_counts())*1.0 :\n",
    "        return pd.Series.mode(x)\n",
    "    else:\n",
    "        return x.value_counts()[:3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emtype(x):\n",
    "    if x.emotion=='sadness':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='fear':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='disgust':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='anger':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='contentment':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='awe':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='amusement':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='excitement':\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"something else\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=df\n",
    "dfemo['emotype']= dfemo.apply(emtype,axis=1)\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d51ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo.groupby([\"art_style\",\"painting\"])[\"emotype\"].agg(modo).reset_index()\n",
    "\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aca6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=type(dfemo.emotype[0])\n",
    "dfemo=dfemo[dfemo[\"emotype\"].apply(lambda x: type(x) !=t )].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo[dfemo.emotype!=\"something else\"].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo['path']= dfemo.apply(lambda x: 'dataset\\\\wikiart\\\\'+x['art_style']+\"\\\\\"+ x['painting']+\".jpg\", axis=1)\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac=1-(dfemo.emotype.value_counts()[1]/dfemo.emotype.value_counts()[0])\n",
    "\n",
    "dfemo = dfemo.drop(dfemo[dfemo['emotype'] == \"positive\"].sample(frac=frac).index)\n",
    "dfemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde6b69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(dfemo.emotype,bins=range(0,3), rwidth=0.8,align=\"left\")\n",
    "plt.title('Histogram of Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c506bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return torchvision.transforms.functional.pad(image, padding, 0, 'constant')\n",
    "\n",
    "# now use it as the replacement of transforms.Pad class\n",
    "transform=torchvision.transforms.Compose([\n",
    "    SquarePad(),\n",
    "    torchvision.transforms.Resize(64),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=torchvision.datasets.ImageFolder(root=\"diffset\",transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed it into a dataloader (batch size 8 here just for demo)\n",
    "train_dataloader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# View some examples\n",
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x).permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7630071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=2, class_emb_size=4):\n",
    "    super().__init__()\n",
    "    \n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=64,           # the target image resolution\n",
    "        in_channels=3 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=3,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(64, 128, 128), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dinemsion\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the unet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c31edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
    "train_dataloader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Our network \n",
    "net = ClassConditionedUnet().to(\"cuda\")\n",
    "\n",
    "# Our loss finction\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(\"cuda\") * 2 - 1 # Data on the GPU (mapped to (-1, 1))\n",
    "        y = y.to(\"cuda\")\n",
    "        noise = torch.randn_like(x)\n",
    "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(\"cuda\")\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, noise) # How close is the output to the noise\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print our the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare random x to start from, plus some desired labels y\n",
    "x = torch.randn(80, 3, 64, 64).to(device)\n",
    "y = torch.tensor([[i]*5 for i in range(2)]).flatten().to(device)\n",
    "\n",
    "# Sampling loop\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
    "\n",
    "    # Update sample with step\n",
    "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "# Show the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8).permute(1,2,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
