{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DDPMScheduler, UNet2DModel, DDPMPipeline, DDIMScheduler,StableDiffusionPipeline,DPMSolverMultistepScheduler\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8874d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ArtEmisv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2144a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modo(x):\n",
    "    if  x.value_counts()[0]>=sum(x.value_counts())*1.0 :\n",
    "        return pd.Series.mode(x)\n",
    "    else:\n",
    "        return x.value_counts()[:3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emtype(x):\n",
    "    if x.emotion=='sadness':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='fear':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='disgust':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='anger':\n",
    "        return \"negative\"\n",
    "    elif x.emotion=='contentment':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='awe':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='amusement':\n",
    "        return \"positive\"\n",
    "    elif x.emotion=='excitement':\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"something else\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=df\n",
    "dfemo['emotype']= dfemo.apply(emtype,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d51ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo.groupby([\"art_style\",\"painting\"])[\"emotype\"].agg(modo).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aca6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=type(dfemo.emotype[0])\n",
    "dfemo=dfemo[dfemo[\"emotype\"].apply(lambda x: type(x) !=t )].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo=dfemo[dfemo.emotype!=\"something else\"].reset_index()\n",
    "dfemo = dfemo.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfemo['path']= dfemo.apply(lambda x: 'dataset\\\\wikiart\\\\'+x['art_style']+\"\\\\\"+ x['painting']+\".jpg\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frac=1-(dfemo.emotype.value_counts()[1]/dfemo.emotype.value_counts()[0])\n",
    "\n",
    "#dfemo = dfemo.drop(dfemo[dfemo['emotype'] == \"positive\"].sample(frac=frac).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(dfemo.emotype,bins=range(0,3), rwidth=0.8,align=\"left\")\n",
    "plt.title('Histogram of Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c506bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return torchvision.transforms.functional.pad(image, padding, 255, 'constant')\n",
    "\n",
    "# now use it as the replacement of transforms.Pad class\n",
    "transform=torchvision.transforms.Compose([\n",
    "    SquarePad(),\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(256),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=torchvision.datasets.ImageFolder(root=\"diffset\",transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed it into a dataloader (batch size 8 here just for demo)\n",
    "train_dataloader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "# View some examples\n",
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x).permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7630071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=2, class_emb_size=2):\n",
    "    super().__init__()\n",
    "    \n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=64,           # the target image resolution\n",
    "        in_channels=3 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=3,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256,256,512,512), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\", \n",
    "            \"AttnUpBlock2D\", \n",
    "            \"UpBlock2D\",      \n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\", \n",
    "            \"UpBlock2D\",\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dinemsion\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the unet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c31edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c635eb-e9b0-4469-a8d4-0cba2150e053",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DDPMPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17984/3598154262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDDPMPipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"google/ddpm-bedroom-256\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mDDIMScheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"google/ddpm-bedroom-256\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_timesteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_inference_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DDPMPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "net=DDPMPipeline.from_pretrained(\"google/ddpm-bedroom-256\").to(\"cuda\")\n",
    "noise_scheduler= DDIMScheduler.from_pretrained(\"google/ddpm-bedroom-256\")\n",
    "noise_scheduler.set_timesteps(num_inference_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
    "train_dataloader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 50\n",
    "\n",
    "# Our network \n",
    "#net = ClassConditionedUnet().to(\"cuda\")\n",
    "\n",
    "# Our loss finction\n",
    "loss_fn = pytorch_msssim.SSIM()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.unet.parameters(), lr=1e-6) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(\"cuda\") *2 -1 # Data on the GPU (mapped to (-1, 1))\n",
    "        y = y.to(\"cuda\")\n",
    "        noise = torch.randn_like(x)\n",
    "        #timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(\"cuda\")\n",
    "        timesteps=torch.randint(0, net.scheduler.num_train_timesteps, (x.shape[0],)).long().to(\"cuda\")\n",
    "        noisy_x = net.scheduler.add_noise(x, noise, timesteps)\n",
    "        \n",
    "        # Get the model prediction\n",
    "        \n",
    "        pred = net.unet(noisy_x, timesteps,y)[0] # Note that we pass in the labels y\n",
    "        \n",
    "        # Calculate the loss\n",
    "        #loss = 1-loss_fn(pred, noise) # How close is the output to the noise\n",
    "        loss=F.mse_loss(pred,noise)\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print our the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare random x to start from, plus some desired labels y\n",
    "x = torch.randn(10, 3, 256, 256).to('cuda')\n",
    "y = torch.tensor([[i]*5 for i in range(2)]).flatten().to('cuda')\n",
    "\n",
    "# Sampling loop\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = net.unet(x, t,y)[\"sample\"]  # Again, note that we pass in our labels y\n",
    "\n",
    "    # Update sample with step\n",
    "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "# Show the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(48, 48))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(0, 255), nrow=8).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0393534-5a87-4ce2-9240-0b865a114fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_pretrained(\"D://EmotionalArtGeneration//pretrained10epoch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
