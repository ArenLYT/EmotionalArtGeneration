{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai import learner\n",
    "from fastai.vision.all import *\n",
    "import diffusers\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import accelerate\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DDPMScheduler, UNet2DModel, DDPMPipeline, DDIMScheduler,StableDiffusionPipeline,DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21366e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]\n",
    "model=load_learner(\"FinalClassifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predimg=fastai.vision.core.PILImage(load_image(r\"D:\\Thesis\\EmotionalArtGeneration\\results\\positive\\87.png\"))\n",
    "predimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(predimg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffusion\n",
    "# modified from https://github.com/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb\n",
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=2, class_emb_size=2):\n",
    "    super().__init__()\n",
    "    \n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=256,           # the target image resolution\n",
    "        in_channels=3 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=3,           # the number of output channels\n",
    "        layers_per_block=4,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256,256,512,512), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\", \n",
    "            \"AttnUpBlock2D\", \n",
    "            \"UpBlock2D\",      \n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\", \n",
    "            \"UpBlock2D\",\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dinemsion\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the unet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)\n",
    "\n",
    "\n",
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=500, beta_schedule='squaredcos_cap_v2')\n",
    "netload=torch.load(\"D://Thesis//EmotionalArtGeneration//epoch10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(20):\n",
    "    # Prepare random x to start from, plus some desired labels y\n",
    "    x = torch.randn(10, 3, 256, 256).to('cuda')\n",
    "    y = torch.tensor([[i]*5 for i in range(2)]).flatten().to('cuda')\n",
    "    # Sampling loop\n",
    "    for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "    \n",
    "        # Get model pred\n",
    "        with torch.no_grad():\n",
    "            residual = netload(x, t,y)  # Again, note that we pass in our labels y\n",
    "    \n",
    "        # Update sample with step\n",
    "        x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "    z=(x+1)/2\n",
    "    p=z.detach().cpu()\n",
    "    for k in range(p.shape[0]):\n",
    "        #result=Image.fromarray(p[i].permute(1,2,0))\n",
    "        if k<5:\n",
    "            torchvision.utils.save_image(p[k],\"results//negative//\"+str(k+(j*10))+\".png\")\n",
    "        else:\n",
    "            torchvision.utils.save_image(p[k],\"results//positive//\"+str(k+j*10)+\".png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
